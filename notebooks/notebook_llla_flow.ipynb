{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fdefdf5-2a0a-4ca7-9672-7e1544aad8a1",
   "metadata": {
    "id": "1fdefdf5-2a0a-4ca7-9672-7e1544aad8a1"
   },
   "source": [
    "# 🧙‍♂️ Sample Generation with Pretrained Model + LLLA\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Jac-Zac/PML_DL_Final_Project/blob/master/notebooks/notebook_llla_flow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5LsZiP4Jfsqw",
   "metadata": {
    "id": "5LsZiP4Jfsqw"
   },
   "source": [
    "### Initial setup ⚙️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ebd6a-207e-4e58-9e53-8eb656a61957",
   "metadata": {
    "id": "036ebd6a-207e-4e58-9e53-8eb656a61957"
   },
   "source": [
    "### If runned locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c880eaf-d325-40af-8b9a-a965d417023e",
   "metadata": {
    "id": "0c880eaf-d325-40af-8b9a-a965d417023e",
    "outputId": "811053c2-f235-4efb-ee65-9a3a5cdf661b"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1c9b39-c109-4f95-85c2-817b6a9650c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install laplace-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hjV2GS6MlwQP",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jaczac/Github/PML_DL_Final_Project\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    import os\n",
    "    !pip install laplace-torch -q \n",
    "    repo_dir = \"PML_DL_Final_Project\"\n",
    "    \n",
    "    if not os.path.exists(repo_dir):\n",
    "        !git clone https://github.com/Jac-Zac/PML_DL_Final_Project.git\n",
    "    else:\n",
    "        print(f\"Repository '{repo_dir}' already exists. Skipping clone.\")\n",
    "\n",
    "        \n",
    "    if os.path.isdir(repo_dir):\n",
    "        %cd $repo_dir\n",
    "        !pip install dotenv -q\n",
    "    else:\n",
    "        print(f\"Directory '{repo_dir}' not found. Please clone the repository first.\")\n",
    "\n",
    "else:\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b0bc4-6bf5-40b0-8af7-5c30a02c1ada",
   "metadata": {
    "id": "064b0bc4-6bf5-40b0-8af7-5c30a02c1ada"
   },
   "source": [
    "### 📦 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f6b33e-bec1-47a2-afe0-c57ca840a622",
   "metadata": {
    "id": "31f6b33e-bec1-47a2-afe0-c57ca840a622"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.models.flow import FlowMatching\n",
    "from src.utils.data import get_dataloaders\n",
    "from src.utils.plots import plot_image_uncertainty_grid\n",
    "from src.utils.environment import get_device, set_seed, load_pretrained_model\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345d2fb-62f0-4d0d-9cfb-12ca609dcba8",
   "metadata": {
    "id": "a345d2fb-62f0-4d0d-9cfb-12ca609dcba8"
   },
   "source": [
    "### 🧪 Setup: Seed and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d158ca43-df3a-45a2-9e9d-47ced73866ff",
   "metadata": {
    "id": "d158ca43-df3a-45a2-9e9d-47ced73866ff"
   },
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "set_seed(seed)\n",
    "device = get_device()\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbc5f8-901d-466b-ad2a-9929037e20b0",
   "metadata": {
    "id": "d2cbc5f8-901d-466b-ad2a-9929037e20b0",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 💡 Image Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b0a39-6cfe-48f1-8cd9-3a0114637c05",
   "metadata": {
    "id": "4e9b0a39-6cfe-48f1-8cd9-3a0114637c05"
   },
   "source": [
    "#### 🛠️ Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b288654b-668c-4ce7-8f10-d862677e29e4",
   "metadata": {
    "id": "b288654b-668c-4ce7-8f10-d862677e29e4"
   },
   "outputs": [],
   "source": [
    "n_samples = 5\n",
    "save_dir = \"samples\"\n",
    "model_name = \"unet\"\n",
    "method = \"flow\"\n",
    "ckpt_path = \"checkpoints/best_model.pth\"  # or use your last checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ba2dda",
   "metadata": {
    "id": "81ba2dda"
   },
   "source": [
    "#### Define Class for QUDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e04153e5-2eb7-4583-a298-faf52bff6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "\n",
    "class UQFlowMatching(FlowMatching):\n",
    "    \"\"\"\n",
    "    Flow Matching with Uncertainty Quantification via Monte Carlo sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size: int = 64, device: torch.device = torch.device(\"cpu\")):\n",
    "        super().__init__(img_size, device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def monte_carlo_covariance_estim(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        t: Tensor,\n",
    "        x_mean: Tensor,\n",
    "        x_var: Tensor,\n",
    "        S: int = 100,\n",
    "        y: Optional[Tensor] = None,\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Perform Monte Carlo sampling to estimate covariance matrix.\n",
    "        Args:\n",
    "            mean_x0: Mean of x_0 estimated by diffusion.\n",
    "            var_x0: Variance of x_0 estimated by propagation.\n",
    "            S: Number of Monte Carlo samples.\n",
    "\n",
    "        Returns:\n",
    "            mc_mean: Empirical mean of samples.\n",
    "            mc_var: Empirical pixel-wise variance of samples.\n",
    "        \"\"\"\n",
    "\n",
    "        std_x = torch.sqrt(torch.clamp(x_var, min=1e-12))\n",
    "        x_samples = [x_mean + std_x * torch.randn_like(x_mean) for _ in range(S)]\n",
    "        v = [model.accurate_forward(x_i, t, y=y) for x_i in x_samples]\n",
    "\n",
    "        x_samples = torch.stack(x_samples, dim=0)  # [S, B, C, H, W]\n",
    "        v = torch.stack(v, dim=0)  # [S, B, C, H, W]\n",
    "\n",
    "        # mean_x = torch.mean(x_samples, dim=(0,1,3,4))  # shape: [C, H, W]\n",
    "        # mean_v = torch.mean(v, dim=(0,1,3,4))          # shape: [C, H, W]\n",
    "\n",
    "        # NOTE: For stability we center it\n",
    "        x_samples = x_samples -  x_samples.mean(dim=0)\n",
    "        v = v - v.mean(dim=0)\n",
    "        \n",
    "        first_term = torch.mean(x_samples * v, dim=0)  # [B, C, H, W]\n",
    "        second_term = x_mean * torch.mean(v, dim=0) # [B, C, H, W]\n",
    "\n",
    "        return first_term - second_term\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_with_uncertainty(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        t_sample_times: Optional[List[int]] = None,\n",
    "        channels: int = 1,\n",
    "        log_intermediate: bool = True,\n",
    "        y: Optional[Tensor] = None,\n",
    "        cov_num_sample: int = 10,\n",
    "        num_steps: int = 10,\n",
    "    ) -> Tuple[List[Tensor], List[Tensor]]:\n",
    "        \"\"\"\n",
    "        Sample with uncertainty tracking and Cov(x, v) estimation.\n",
    "\n",
    "        Returns:\n",
    "            intermediates: List of sampled images at given steps.\n",
    "            uncertainties: List of per-pixel variance maps at those steps.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        batch_size = 1 if y is None else y.size(0)\n",
    "\n",
    "        x_t = torch.randn(\n",
    "            batch_size, channels, self.img_size, self.img_size, device=self.device\n",
    "        )\n",
    "        x_t_mean = x_t.clone()\n",
    "        x_t_var = torch.zeros_like(x_t)\n",
    "        cov_t = torch.zeros_like(x_t)\n",
    "\n",
    "        intermediates, uncertainties = [], []\n",
    "\n",
    "        dt = 1.0 / num_steps\n",
    "        \n",
    "        for i in tqdm(range(num_steps), desc=\"Steps\", leave=False):\n",
    "        # for i in range(num_steps):\n",
    "            t = torch.full((batch_size,), i * dt, device=self.device, dtype=torch.long)\n",
    "\n",
    "            #################################\n",
    "            # Predict noise and its variance\n",
    "            v_mean, v_var = model(x_t, t, y=y)  # mean and variance of noise\n",
    "\n",
    "            v_t = v_mean + torch.sqrt(v_var) * torch.randn_like(v_mean)\n",
    "            x_succ = x_t + dt * v_t\n",
    "\n",
    "            # Mean\n",
    "            x_succ_mean = x_t_mean + dt * v_mean\n",
    "\n",
    "            # Variance\n",
    "            x_succ_var = x_t_var + dt**2 * v_var + 2 * dt * cov_t\n",
    "\n",
    "            # print(f\"\\nStep {i}\")\n",
    "            # print(\"v_var mean:\", v_var.mean().item(), \"std:\", v_var.std().item())\n",
    "            # print(\"covariance mean:\", cov_t.mean().item(), \"std:\", cov_t.std().item())\n",
    "            # print(\"x_t_var mean:\", x_t_var.mean().item(), \"std:\", x_t_var.std().item())\n",
    "            # print(\"x_succ_var mean:\", x_succ_var.mean().item(), \"std:\", x_succ_var.std().item())\n",
    "\n",
    "            # Covariance estimation with Monte Carlo\n",
    "            covariance = self.monte_carlo_covariance_estim(\n",
    "               model=model,\n",
    "               t=t + dt,\n",
    "               x_mean=x_succ_mean,\n",
    "               x_var=x_succ_var,\n",
    "               S=cov_num_sample,\n",
    "               y=y,\n",
    "            )\n",
    "\n",
    "            # Log intermediate images\n",
    "            if log_intermediate and t_sample_times and i in t_sample_times:\n",
    "                intermediates.append(self.transform_sampled_image(x_t.clone()))\n",
    "                uncertainties.append(x_t_var.clone().cpu())  # per-pixel variance\n",
    "\n",
    "            x_t = x_succ\n",
    "            x_t_mean = x_succ_mean\n",
    "            x_t_var = x_succ_var\n",
    "            cov_t = covariance\n",
    "\n",
    "        uncertainties = torch.stack(uncertainties)  # [num_steps, B, C, H, W]\n",
    "\n",
    "        model.train()\n",
    "        return intermediates, uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f986a39",
   "metadata": {
    "id": "9f986a39"
   },
   "source": [
    "### 💪 Fit Laplace approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "P62LzRyaxfBn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P62LzRyaxfBn",
    "outputId": "472dc8b5-994e-4205-c92b-8ebb80bff747"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to write /Users/jaczac/.config/netrc/netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacopozac\u001b[0m (\u001b[33mjac-zac\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6db9f8fa097492783b0159750914fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fitting Laplace:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m mnist_config.data.image_size = \u001b[32m28\u001b[39m  \u001b[38;5;66;03m# MNIST image size\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Wrap diffusion model with your Custom Model for Laplace last layer approx\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# NOTE: Automatically call fit\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m laplace_model = \u001b[43mLaplaceApproxModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflow_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmnist_config\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLaplace fitting completed on last layer of the diffusion model.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/src/models/llla_model.py:43\u001b[39m, in \u001b[36mLaplaceApproxModel.__init__\u001b[39m\u001b[34m(self, diff_model, dataloader, args, config)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mself\u001b[39m.conv_out_la = DiagLaplace(\n\u001b[32m     31\u001b[39m     nn.Sequential(\n\u001b[32m     32\u001b[39m         \u001b[38;5;28mself\u001b[39m.conv_out, nn.Flatten(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     backend=BackPackEF,  \u001b[38;5;66;03m# Curvature estimator backend\u001b[39;00m\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Fit the Laplace approximation using the training data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/src/models/llla_model.py:80\u001b[39m, in \u001b[36mLaplaceApproxModel.fit\u001b[39m\u001b[34m(self, train_loader, override)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Size of the data loader\u001b[39;00m\n\u001b[32m     78\u001b[39m N = \u001b[38;5;28mlen\u001b[39m(train_loader.dataset)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFitting Laplace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv_out_la\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move batch to the appropriate device\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/src/utils/data.py:98\u001b[39m, in \u001b[36mFlowMatchingMNIST.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m\n\u001b[32m     90\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Any]:\n\u001b[32m     91\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m        x_t: interpolated image between x0 and x1 at time t\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     96\u001b[39m \u001b[33;03m        label: digit label\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     image, label = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     99\u001b[39m     x1 = image * \u001b[32m2.0\u001b[39m - \u001b[32m1.0\u001b[39m  \u001b[38;5;66;03m# scale to [-1, 1]\u001b[39;00m\n\u001b[32m    100\u001b[39m     x0 = torch.randn_like(x1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:277\u001b[39m, in \u001b[36mNormalize.forward\u001b[39m\u001b[34m(self, tensor)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) -> Tensor:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m \u001b[33;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:350\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch.Tensor):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/PML_DL_Final_Project/.venv/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:922\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(tensor, mean, std, inplace)\u001b[39m\n\u001b[32m    920\u001b[39m mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n\u001b[32m    921\u001b[39m std = torch.as_tensor(std, dtype=dtype, device=tensor.device)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, leading to division by zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mean.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from src.models.llla_model import LaplaceApproxModel\n",
    "from src.utils.data import get_llla_dataloader\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "model_kwargs={\n",
    "    \"num_classes\": num_classes, \"time_emb_dim\": 128,\n",
    "    # NOTE: We are currently using different time embedding because of a small bug but it is fine\n",
    "    \"time_embedding_type\": \"mlp\" if method == \"flow\" else \"sinusoidal\"\n",
    "}\n",
    "\n",
    "# Load pretrained MAP model using best checkpoint\n",
    "flow_model = load_pretrained_model(\n",
    "    model_name=\"unet\",\n",
    "    ckpt_path=\"jac-zac/diffusion-project/best-model:v91\",\n",
    "    device=device,\n",
    "    model_kwargs=model_kwargs,\n",
    "    use_wandb=True,\n",
    ")\n",
    "\n",
    "# 2️⃣ Prepare data loaders for the Laplace fit\n",
    "train_loader, _ = get_llla_dataloader(batch_size=128, mode = \"flow\")\n",
    "\n",
    "mnist_config = SimpleNamespace()\n",
    "mnist_config.data = SimpleNamespace()\n",
    "mnist_config.data.image_size = 28  # MNIST image size\n",
    "\n",
    "# Wrap diffusion model with your Custom Model for Laplace last layer approx\n",
    "# NOTE: Automatically call fit\n",
    "laplace_model = LaplaceApproxModel(\n",
    "    flow_model, train_loader, args=None, config=mnist_config\n",
    ")\n",
    "\n",
    "print(\"Laplace fitting completed on last layer of the diffusion model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b667cd2",
   "metadata": {
    "id": "3b667cd2",
    "lines_to_next_cell": 0
   },
   "source": [
    "<!-- #region id=\"1d2b6a2d\" -->\n",
    "### 💨 Initialize Flow Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xnCp4gh00K1R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "xnCp4gh00K1R",
    "outputId": "f45c833f-5332-4c0f-94f0-e0d954530191"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# NOTE: total steps and num intermediate have to match in this flow matching implementation\n",
    "total_steps = 50\n",
    "num_intermediate = 10\n",
    "\n",
    "# Initialize uncertainty-aware diffusion (same interface as base class)\n",
    "flow = UQFlowMatching(img_size=28, device=device)\n",
    "\n",
    "all_samples_grouped, uncertainties = plot_image_uncertainty_grid(\n",
    "        model=laplace_model,\n",
    "        method_instance=flow,\n",
    "        num_intermediate=num_intermediate,\n",
    "        n=1,\n",
    "        total_steps=total_steps,\n",
    "        save_dir=save_dir,\n",
    "        device=device,\n",
    "        num_classes=num_classes,\n",
    "        cov_num_sample=100,\n",
    "        # uq_cmp = \"viridis\",\n",
    "        uq_cmp = \"grey\",\n",
    "    )\n",
    "\n",
    "# Display samples grid\n",
    "out_path_img = os.path.join(save_dir, \"all_samples_grid.png\")\n",
    "display(Image.open(out_path_img))\n",
    "\n",
    "# Display uncertainties grid\n",
    "out_path_unc = os.path.join(save_dir, \"all_uncertainties_grid.png\")\n",
    "display(Image.open(out_path_unc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4e232-2bca-4ec8-9d32-d9d2773740e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "0bf4e232-2bca-4ec8-9d32-d9d2773740e4",
    "outputId": "f4d631f5-0706-4f9d-8341-4630ba66d292"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.plots import plot_image_uncertainty_grid\n",
    "# Sum over the last two dimensions (28x28)\n",
    "\n",
    "# If I need to get just one of the samples\n",
    "sums = uncertainties[:, 0].sum(dim=[-1, -2])  # shape: [10, 1, 1]\n",
    "# sums = uncertainties.sum(dim=[-1, -2])  # shape: [10, 1, 1]\n",
    "\n",
    "# Flatten to shape\n",
    "sums_flat = sums.view(num_intermediate)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(num_intermediate), sums_flat.tolist(), marker='o', linestyle='-')\n",
    "plt.title(\"Sum of last two dimensions per item in first dimension\")\n",
    "plt.xlabel(\"Index in first dimension\")\n",
    "plt.ylabel(\"Sum over 28x28\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
