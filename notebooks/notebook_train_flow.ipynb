{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jac-Zac/PML_DL_Final_Project/blob/main/notebooks/notebook_train_flow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fdefdf5-2a0a-4ca7-9672-7e1544aad8a1",
      "metadata": {
        "id": "1fdefdf5-2a0a-4ca7-9672-7e1544aad8a1"
      },
      "source": [
        "# üßô‚Äç‚ôÇÔ∏è Training diffusion model\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Jac-Zac/PML_DL_Final_Project/blob/master/notebooks/notebook_train_flow.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5LsZiP4Jfsqw",
      "metadata": {
        "id": "5LsZiP4Jfsqw",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Initial setup ‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git fetch https://github.com/Jac-Zac/PML_DL_Final_Project.git"
      ],
      "metadata": {
        "id": "ZwWYovar-v7m"
      },
      "id": "ZwWYovar-v7m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hjV2GS6MlwQP",
      "metadata": {
        "id": "hjV2GS6MlwQP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "repo_dir = \"PML_DL_Final_Project\"\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "    !git clone https://github.com/Jac-Zac/PML_DL_Final_Project.git\n",
        "else:\n",
        "    print(f\"Repository '{repo_dir}' already exists. Skipping clone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8amRYEyKfb8n",
      "metadata": {
        "id": "8amRYEyKfb8n"
      },
      "outputs": [],
      "source": [
        "if os.path.isdir(repo_dir):\n",
        "    %cd $repo_dir\n",
        "    !pip install dotenv -q\n",
        "else:\n",
        "    print(f\"Directory '{repo_dir}' not found. Please clone the repository first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "064b0bc4-6bf5-40b0-8af7-5c30a02c1ada",
      "metadata": {
        "id": "064b0bc4-6bf5-40b0-8af7-5c30a02c1ada"
      },
      "source": [
        "### üì¶ Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31f6b33e-bec1-47a2-afe0-c57ca840a622",
      "metadata": {
        "id": "31f6b33e-bec1-47a2-afe0-c57ca840a622",
        "lines_to_next_cell": 0
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from src.train.train import train\n",
        "from src.utils.data import get_dataloaders\n",
        "# from src.utils.plots import plot_image_grid\n",
        "from src.utils.environment import get_device, set_seed, load_pretrained_model\n",
        "\n",
        "# train\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "from src.utils.environment import load_checkpoint\n",
        "from src.utils.wandb import (\n",
        "    initialize_wandb,\n",
        "    log_epoch_metrics,\n",
        "    log_sample_grid,\n",
        "    log_training_step,\n",
        "    save_best_model_artifact,\n",
        ")\n",
        "\n",
        "import src.utils.wandb\n",
        "\n",
        "\n",
        "# Since on a notebook we can have nicer bars\n",
        "from tqdm.notebook import tqdm as tqdm_notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cdf6e7e",
      "metadata": {
        "id": "0cdf6e7e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "675f2006",
      "metadata": {
        "id": "675f2006"
      },
      "source": [
        "## Flow Matching implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0937ab",
      "metadata": {
        "id": "fe0937ab"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "\n",
        "\n",
        "class FlowMatching:\n",
        "    def __init__(self, img_size: int = 64, device: torch.device = torch.device(\"cpu\")):\n",
        "        self.img_size = img_size\n",
        "        self.device = device\n",
        "\n",
        "    def _sample_timesteps(self, batch_size: int) -> Tensor:\n",
        "        return torch.rand(batch_size, device=self.device)\n",
        "\n",
        "    def perform_training_step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        x0: Tensor,  # noise ~ N(0,I)\n",
        "        x1: Tensor,  # data in [-1,1]\n",
        "        y: Optional[Tensor] = None,\n",
        "    ) -> Tensor:\n",
        "        B = x0.size(0)\n",
        "        t = self._sample_timesteps(B)\n",
        "        t4 = t.view(-1, 1, 1, 1)  # shape: [B, 1, 1, 1]\n",
        "        x_t = (1 - t4) * x0 + t4 * x1  # linear OT path\n",
        "\n",
        "        # True velocity & normalization\n",
        "        dx = x1 - x0\n",
        "\n",
        "        u_t = model(x_t, t, y=y)\n",
        "        assert u_t.shape == dx.shape\n",
        "\n",
        "        # Time-weighted MSE loss\n",
        "        return ((u_t - dx).pow(2)).mean()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    # def sample(\n",
        "    #     self,\n",
        "    #     model: nn.Module,\n",
        "    #     x_init: Optional[Tensor] = None,\n",
        "    #     steps: int = 100,\n",
        "    #     y: Optional[Tensor] = None,\n",
        "    #     log_intermediate: bool = False,\n",
        "    #     t_sample_skip = 1,\n",
        "    #     t_sample_times: Optional[list[int]] = None,\n",
        "    # ) -> list[Tensor]:\n",
        "    #     model.eval()\n",
        "    #     B = (\n",
        "    #         y.shape[0]\n",
        "    #         if y is not None\n",
        "    #         else (x_init.shape[0] if x_init is not None else 1)\n",
        "    #     )\n",
        "    #     C = x_init.shape[1] if x_init is not None else 1\n",
        "    #     x_t = (\n",
        "    #         x_init.to(self.device)\n",
        "    #         if x_init is not None\n",
        "    #         else torch.randn(B, C, self.img_size, self.img_size, device=self.device)\n",
        "    #     )\n",
        "\n",
        "    #     results = []\n",
        "    #     dt = 1.0 / steps\n",
        "    #     for i in range(steps):\n",
        "    #         t = torch.full((B,), i / steps, device=self.device)\n",
        "    #         u_t = model(x_t, t, y=y)\n",
        "    #         x_t = x_t + u_t * dt\n",
        "    #         if t_sample_times and i in t_sample_times:\n",
        "    #             results.append(self.transform_sampled_image(x_t.clone()))\n",
        "    #         elif i % t_sample_skip == 0:\n",
        "    #             results.append(self.transform_sampled_image(x_t.clone()))\n",
        "\n",
        "    #     results.append(self.transform_sampled_image(x_t))\n",
        "    #     return results\n",
        "\n",
        "    def sample(\n",
        "      self,\n",
        "      model: nn.Module,\n",
        "      x_init: Optional[Tensor] = None,\n",
        "      steps: int = 100,\n",
        "      y: Optional[Tensor] = None,\n",
        "      log_intermediate: bool = False,\n",
        "      t_sample_times: Optional[list[int]] = None,\n",
        "    ) -> list[Tensor]:\n",
        "\n",
        "      model.eval()\n",
        "      B = y.shape[0] if y is not None else (x_init.shape[0] if x_init is not None else 1)\n",
        "      C = x_init.shape[1] if x_init is not None else 1\n",
        "      x_t = (\n",
        "          x_init.to(self.device)\n",
        "          if x_init is not None\n",
        "          else torch.randn(B, C, self.img_size, self.img_size, device=self.device)\n",
        "      )\n",
        "\n",
        "      results = []\n",
        "      dt = 1.0 / steps\n",
        "\n",
        "      sample_step_indices = set()\n",
        "      if log_intermediate and t_sample_times:\n",
        "          sample_step_indices = set([steps - t for t in t_sample_times])\n",
        "\n",
        "      for i in range(steps):\n",
        "          t = torch.full((B,), i / steps, device=self.device)\n",
        "          v = model(x_t, t, y=y)\n",
        "          x_t = x_t + v * dt\n",
        "\n",
        "          if (i + 1) in sample_step_indices:\n",
        "              results.append(self.transform_sampled_image(x_t.clone()))\n",
        "\n",
        "      return results\n",
        "\n",
        "    @staticmethod\n",
        "    def transform_sampled_image(image: Tensor) -> Tensor:\n",
        "        return (image.clamp(-1, 1) + 1) / 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23ac9c3b-0116-4082-bfe9-9ddeac4581b3",
      "metadata": {
        "id": "23ac9c3b-0116-4082-bfe9-9ddeac4581b3"
      },
      "source": [
        "### üõ†Ô∏è Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cce8cf-ffe7-43a1-9365-11448299f119",
      "metadata": {
        "id": "d3cce8cf-ffe7-43a1-9365-11448299f119"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "batch_size = 128\n",
        "learning_rate = 2e-3\n",
        "seed = 1337\n",
        "checkpoint_path = \"checkpoints/last.ckpt\"\n",
        "model_name = \"unet\"\n",
        "method = \"flow\"  # or \"flow\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a345d2fb-62f0-4d0d-9cfb-12ca609dcba8",
      "metadata": {
        "id": "a345d2fb-62f0-4d0d-9cfb-12ca609dcba8"
      },
      "source": [
        "### üß™ Setup: Seed and Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d158ca43-df3a-45a2-9e9d-47ced73866ff",
      "metadata": {
        "id": "d158ca43-df3a-45a2-9e9d-47ced73866ff"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "device = get_device()\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a864b6ab-979f-4e2c-a83b-c2f096be6c2a",
      "metadata": {
        "id": "a864b6ab-979f-4e2c-a83b-c2f096be6c2a",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## üß† Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3544b19-39dd-4a46-a37a-53a761878804",
      "metadata": {
        "id": "a3544b19-39dd-4a46-a37a-53a761878804"
      },
      "source": [
        "#### üì• Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d388d2-7c0b-4657-9ec2-8d7de4dd451a",
      "metadata": {
        "id": "56d388d2-7c0b-4657-9ec2-8d7de4dd451a"
      },
      "outputs": [],
      "source": [
        "# Returns DataLoaders that yield (image, timestep, label)\n",
        "train_loader, val_loader = get_dataloaders(batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4103982-141a-4886-9f33-969cc2b5aab4",
      "metadata": {
        "id": "c4103982-141a-4886-9f33-969cc2b5aab4"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(\n",
        "    model,\n",
        "    dataloader,\n",
        "    optimizer,\n",
        "    method_instance,\n",
        "    device,\n",
        "    use_wandb,\n",
        "    grad_clip: float = 1.0,\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, labels in tqdm_notebook(dataloader, desc=\"Training\", leave=False):\n",
        "        images = images.to(device).mul_(2).sub_(1)  # transforms image values from [0,1] ‚Üí [-1,1]\n",
        "        y = labels.to(device)\n",
        "        x0 = torch.randn_like(images)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss = method_instance.perform_training_step(model=model, x0=x0, x1=images, y=y)\n",
        "        loss.backward()\n",
        "\n",
        "        if grad_clip is not None:\n",
        "            clip_grad_norm_(model.parameters(), max_norm=grad_clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_val = loss.item()\n",
        "        total_loss += loss_val\n",
        "\n",
        "        if use_wandb:\n",
        "            log_training_step(loss_val)\n",
        "\n",
        "    return total_loss / max(1, len(dataloader))  #average loss per batch for the epoch\n",
        "\n",
        "\n",
        "def validate(model, val_loader, method_instance, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm_notebook(val_loader, desc=\"Validating\", leave=False):\n",
        "            images = images.to(device).mul_(2).sub_(1)\n",
        "            y = labels.to(device)\n",
        "            x0 = torch.randn_like(images)\n",
        "\n",
        "            loss = method_instance.perform_training_step(model=model, x0=x0, x1=images, y=y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "\n",
        "def train(\n",
        "    num_epochs: int,\n",
        "    device: torch.device,\n",
        "    dataloader,\n",
        "    val_loader,\n",
        "    learning_rate: float = 1e-3,\n",
        "    use_wandb: bool = False,\n",
        "    checkpoint_path: Optional[str] = None,\n",
        "    model_name: str = \"unet\",\n",
        "    model_kwargs: Optional[dict] = None,\n",
        "    method: str = \"flow\",\n",
        "):\n",
        "    model_kwargs = model_kwargs or {}\n",
        "\n",
        "    # Initialize scheduler ahead of load_checkpoint\n",
        "    dummy_optimizer = torch.optim.Adam([torch.zeros(1)], lr=learning_rate)\n",
        "    scheduler = CosineAnnealingLR(dummy_optimizer, T_max=num_epochs, eta_min=1e-5)\n",
        "\n",
        "    model, optimizer, _, start_epoch, best_val_loss = load_checkpoint(\n",
        "        model_name=model_name,\n",
        "        checkpoint_path=checkpoint_path,\n",
        "        device=device,\n",
        "        optimizer_class=torch.optim.Adam,\n",
        "        optimizer_kwargs={\"lr\": learning_rate},\n",
        "        model_kwargs=model_kwargs,\n",
        "        scheduler=scheduler,\n",
        "    )\n",
        "\n",
        "    # Re-create scheduler with real optimizer if not loaded\n",
        "    if not hasattr(scheduler, \"optimizer\") or scheduler.optimizer is not optimizer:\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-5)\n",
        "\n",
        "    # Initialize method instance\n",
        "    if method == \"diffusion\":\n",
        "        # method_instance = Diffusion(img_size=28, device=device)\n",
        "        print(\"Ti sei scordato di cambiare in flow matching\")\n",
        "    elif method == \"flow\":\n",
        "        method_instance = FlowMatching(img_size=28, device=device)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported method: {method}\")\n",
        "\n",
        "    wandb_run = None\n",
        "    if use_wandb:\n",
        "        wandb_run = initialize_wandb(\n",
        "            project=\"flow-project\",\n",
        "            config={\n",
        "                \"epochs\": num_epochs,\n",
        "                \"lr\": learning_rate,\n",
        "                \"model\": model_name,\n",
        "                \"num_classes\": model_kwargs.get(\"num_classes\"),\n",
        "                \"method\": method,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        train_loss = train_one_epoch(\n",
        "            model, dataloader, optimizer, method_instance, device, use_wandb\n",
        "        )\n",
        "        val_loss = validate(model, val_loader, method_instance, device)\n",
        "\n",
        "        scheduler.step()\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "        print(\n",
        "            f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | LR: {current_lr:.6f}\"\n",
        "        )\n",
        "\n",
        "        # Check if this is the best model\n",
        "        is_best = val_loss < best_val_loss\n",
        "        if is_best:\n",
        "            best_val_loss = val_loss\n",
        "\n",
        "        if use_wandb:\n",
        "            log_epoch_metrics(epoch, train_loss, val_loss, current_lr)\n",
        "            log_sample_grid(model, method_instance, num_samples=5, num_timesteps=6)\n",
        "\n",
        "            # Save best model artifact when we find a new best\n",
        "            if is_best:\n",
        "                save_best_model_artifact(\n",
        "                    model=model,\n",
        "                    optimizer=optimizer,\n",
        "                    scheduler=scheduler,\n",
        "                    epoch=epoch,\n",
        "                    val_loss=val_loss,\n",
        "                    train_loss=train_loss,\n",
        "                )\n",
        "\n",
        "    if wandb_run:\n",
        "        wandb_run.finish()\n",
        "\n",
        "    print(f\"\\nTraining complete. Best validation loss: {best_val_loss:.4f}\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "erg-BeFVCApH"
      },
      "id": "erg-BeFVCApH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "296d3d48-2e41-4c2a-a796-c9ae5e577eac",
      "metadata": {
        "id": "296d3d48-2e41-4c2a-a796-c9ae5e577eac"
      },
      "outputs": [],
      "source": [
        "# NOTE: Currently assumes 10 classes are hardcoded\n",
        "num_classes = 10\n",
        "model_kwargs = {\"num_classes\": num_classes}\n",
        "\n",
        "\n",
        "# NOTE: Instead of using train directly you can write here your custom traiing code\n",
        "# You can take inspiration from train to see how the checkpoints are saved\n",
        "\n",
        "# NOTE: You can also directly copy all the code from train a cell above this and modify it inside the notebook\n",
        "# similarly to what was done for the Flow Matching Class\n",
        "\n",
        "# But if you use it directly you can directly use model you have from the train\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flow_model = train(\n",
        "    num_epochs=epochs,\n",
        "    device= device,\n",
        "    dataloader = train_loader,\n",
        "    val_loader = val_loader,\n",
        "    use_wandb = True,\n",
        "    checkpoint_path = checkpoint_path,\n",
        "    model_kwargs = model_kwargs,\n",
        "    method = method\n",
        ")"
      ],
      "metadata": {
        "id": "s14jJ8YTQeCU"
      },
      "id": "s14jJ8YTQeCU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d2cbc5f8-901d-466b-ad2a-9929037e20b0",
      "metadata": {
        "id": "d2cbc5f8-901d-466b-ad2a-9929037e20b0",
        "lines_to_next_cell": 0
      },
      "source": [
        "## üí° Image Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e9b0a39-6cfe-48f1-8cd9-3a0114637c05",
      "metadata": {
        "id": "4e9b0a39-6cfe-48f1-8cd9-3a0114637c05"
      },
      "source": [
        "#### üõ†Ô∏è Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b288654b-668c-4ce7-8f10-d862677e29e4",
      "metadata": {
        "id": "b288654b-668c-4ce7-8f10-d862677e29e4"
      },
      "outputs": [],
      "source": [
        "n_samples = 5     #number of classes I want to sample\n",
        "save_dir = \"samples\"\n",
        "max_steps = 1000\n",
        "model_name = \"unet\"\n",
        "num_timesteps = 6\n",
        "\n",
        "ckpt_path = \"checkpoints/best_model.pth\"  # or use your last checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_flowmatching_grid(model, flow_matching, n, max_steps, save_dir, device, num_classes):\n",
        "    \"\"\"\n",
        "    Generates samples at intermediate steps and plots a grid where\n",
        "    rows = samples and columns = timesteps for Flow Matching.\n",
        "\n",
        "    Args:\n",
        "        model: The flow matching model.\n",
        "        flow_matching: The FlowMatching sampling object.\n",
        "        n (int): Number of samples to generate.\n",
        "        max_steps (int): Maximum flow steps.\n",
        "        save_dir (str): Directory to save the output image.\n",
        "        device: Torch device to run the model on.\n",
        "        num_classes: Number of classes for label conditioning.\n",
        "    \"\"\"\n",
        "    num_intermediate = 5  # This will give us 6 time points (including start and end)\n",
        "    intermediate_steps = np.linspace(\n",
        "        0, max_steps, num_intermediate + 1, dtype=int\n",
        "    ).tolist()  # Changed to go from 0 to max_steps\n",
        "\n",
        "    # Generate labels: 0, 1, ..., n-1 modulo num_classes\n",
        "    y = torch.arange(n) % num_classes\n",
        "    y = y.to(device)\n",
        "\n",
        "    # Sample images\n",
        "    all_samples_grouped = flow_matching.sample(\n",
        "        model,\n",
        "        t_sample_times=intermediate_steps,\n",
        "        log_intermediate=True,\n",
        "        y=y,\n",
        "        steps=max_steps,\n",
        "    )\n",
        "    print(f\"Generated {n} samples with labels {y.tolist()}\")\n",
        "    print(f\"Number of sampled time points: {len(all_samples_grouped)}\")\n",
        "    print(f\"Requested timesteps: {intermediate_steps}\")\n",
        "\n",
        "    # Convert list of tensors to tensor\n",
        "    # Each element in all_samples_grouped has shape [B, C, H, W]\n",
        "    stacked = torch.stack(all_samples_grouped)  # shape: [T, B, C, H, W]\n",
        "    print(f\"Stacked samples shape: {stacked.shape}\")\n",
        "\n",
        "    # Permute dimensions to [B, T, C, H, W]\n",
        "    permuted = stacked.permute(1, 0, 2, 3, 4)\n",
        "    print(f\"Permuted samples shape: {permuted.shape}\")\n",
        "\n",
        "    # Flatten to [B*T, C, H, W]\n",
        "    flat_samples = permuted.reshape(-1, *permuted.shape[2:])\n",
        "    print(f\"Flat samples shape: {flat_samples.shape}\")\n",
        "\n",
        "    # Plot\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    out_path = os.path.join(save_dir, \"all_samples_grid.png\")\n",
        "\n",
        "    num_samples = n\n",
        "    num_timesteps = len(all_samples_grouped)  # Use actual number of returned timesteps\n",
        "    expected_images = num_samples * num_timesteps\n",
        "    print(f\"Expecting {expected_images} images, got {len(flat_samples)}\")\n",
        "\n",
        "    assert len(flat_samples) == expected_images, \\\n",
        "        f\"Mismatch in image count. Expected {expected_images}, got {len(flat_samples)}\"\n",
        "\n",
        "    fig, axes = plt.subplots(\n",
        "        num_samples, num_timesteps,\n",
        "        figsize=(1.5 * num_timesteps, 1.5 * num_samples)\n",
        "    )\n",
        "\n",
        "    if num_samples == 1:\n",
        "        axes = np.expand_dims(axes, 0)\n",
        "    if num_timesteps == 1:\n",
        "        axes = np.expand_dims(axes, 1)\n",
        "\n",
        "    for row in range(num_samples):\n",
        "        for col in range(num_timesteps):\n",
        "            idx = row * num_timesteps + col\n",
        "            img = flat_samples[idx]\n",
        "            if isinstance(img, torch.Tensor):\n",
        "                img = img.squeeze().cpu().numpy()\n",
        "                if img.ndim == 3:  # If color image, transpose to HWC\n",
        "                    img = img.transpose(1, 2, 0)\n",
        "\n",
        "            ax = axes[row, col]\n",
        "            ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None)\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "            if row == 0:\n",
        "                # Show actual step number instead of index\n",
        "                ax.set_title(f\"t={intermediate_steps[col]}\", fontsize=10)\n",
        "            if col == 0:\n",
        "                ax.set_ylabel(f\"Sample {row+1}\", fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "KswEqRtOBUFh"
      },
      "id": "KswEqRtOBUFh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# flow = FlowMatching(img_size=28, device = device)\n",
        "\n",
        "# üí´ Create diffusion sampler\n",
        "flow = FlowMatching(img_size=28, device=device)\n",
        "\n",
        "wandb.init()\n",
        "\n",
        "log_sample_grid(model= flow_model, method_instance=flow, num_samples=n_samples, num_timesteps=num_timesteps, max_timesteps=max_steps)\n",
        "\n",
        "\n",
        "plot_flowmatching_grid(\n",
        "    flow_model,\n",
        "    flow,\n",
        "    n=n_samples,\n",
        "    max_steps=max_steps,\n",
        "    save_dir=save_dir,\n",
        "    device=device,\n",
        "    num_classes=num_classes,\n",
        ")\n",
        "\n",
        "out_path = os.path.join(save_dir, \"all_samples_grid.png\")\n",
        "display(Image.open(out_path))\n",
        "\n",
        "\n",
        "\n",
        "# NOTE: Currently assumes 10 classes are hardcoded\n",
        "#see in to plot_image to make it for flow amtching\n",
        "\n",
        "# plot_image_grid(\n",
        "#     model,\n",
        "#     flow,\n",
        "#     n=n_samples,\n",
        "#     max_steps=max_steps,\n",
        "#     save_dir=save_dir,\n",
        "#     device=device\n",
        "#     num_classes=num_classes\n",
        "# )\n",
        "\n",
        "# But if you use it directly you can directly use model you have from the train\n",
        "\n",
        "# model = train(\n",
        "# num_epochs=epochs,\n",
        "# device=device,\n",
        "# dataloader=train_loader,\n",
        "# val_loader=val_loader,\n",
        "# learning_rate=learning_rate,\n",
        "# use_wandb=True,\n",
        "# checkpoint_path=checkpoint,\n",
        "# model_name=model_name,\n",
        "# model_kwargs=model_kwargs,\n",
        "# method=method,\n",
        "# )\n"
      ],
      "metadata": {
        "id": "g9uJCuiiPknZ"
      },
      "id": "g9uJCuiiPknZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}